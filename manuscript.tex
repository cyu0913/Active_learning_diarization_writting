%%
%% Copyright 2007, 2008, 2009 Elsevier Ltd
%%
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%%
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%%
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%%

%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references
%% SP 2008/03/01
%%
%%
%%
%% $Id: elsarticle-template-harv.tex 4 2009-10-24 08:22:58Z rishi $
%%
%%
%\documentclass[preprint,authoryear, review,12pt]{elsarticle}
\documentclass[final,authoryear,5p,times,twocolumn]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,authoryear,1p,times]{elsarticle}
%% \documentclass[final,authoryear,1p,times,twocolumn]{elsarticle}
%% \documentclass[final,authoryear,3p,times]{elsarticle}
%% \documentclass[final,authoryear,3p,times,twocolumn]{elsarticle}
%% \documentclass[final,authoryear,5p,times]{elsarticle}
%% \documentclass[final,authoryear,5p,times,twocolumn]{elsarticle}

%% if you use PostScript figures in your article
%% use the graphics package for simple commands
%% \usepackage{graphics}
%% or use the graphicx package for more complicated commands
\usepackage{graphicx}
%% or use the epsfig package if you prefer to use the old commands
%% \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
%\usepackage{amssymb}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers after \end{frontmatter}.
%% \usepackage{lineno}

%% natbib.sty is loaded by default. However, natbib options can be
%% provided with \biboptions{...} command. Following options are
%% valid:

\usepackage{lineno}

%\usepackage{amssymb,amsmath,epsfig,xspace,psfrag,mathrsfs}
%\usepackage{ragged2e,multirow,multicol,lscape,booktabs,arydshln,amsmath,latexsym,amssymb}
% \usepackage{cite}
%\usepackage[pdftex]{graphicx}
%\usepackage[dvips]{graphicx}
%\usepackage[cmex10]{amsmath}
%\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage{epsfig}
%\usepackage{ragged2e}
%\usepackage{booktabs}
%\usepackage{arydshln}
%\usepackage{xspace}
%\usepackage{psfrag}
%\usepackage[textfont={rm,sl}]{caption}
%\usepackage[textfont={sc,sc}]{caption}
%\usepackage{multirow,multicol,lscape,booktabs,latexsym}
%\usepackage{mathrsfs}
%\usepackage{afterpage}
%\usepackage{xspace}
%\usepackage{rotate}
%\usepackage[rotateleft]{rotating}
%\usepackage{endnotes}
%\usepackage{soul}
%\usepackage{upgreek}
%\usepackage{bbding}

\usepackage{amsmath,epsfig,latexsym, amssymb}
\usepackage{colortbl, booktabs, ctable}
\usepackage{algpseudocode,algorithm2e}
%\usepackage{stfloats}
%\graphicspath{{figs/}}
%\newcommand{\vectornorm}[1]{\left|\left|#1\right|\right|} 

%%   round  -  round parentheses are used (default)
%%   square -  square brackets are used   [option]
%%   curly  -  curly braces are used      {option}
%%   angle  -  angle brackets are used    <option>
%%   semicolon  -  multiple citations separated by semi-colon (default)
%%   colon  - same as semicolon, an earlier confusion
%%   comma  -  separated by comma
%%   authoryear - selects author-year citations (default)
%%   numbers-  selects numerical citations
%%   super  -  numerical citations as superscripts
%%   sort   -  sorts multiple citations according to order in ref. list
%%   sort&compress   -  like sort, but also compresses numerical citations
%%   compress - compresses without sorting
%%   longnamesfirst  -  makes first citation full author list
%%
%% \biboptions{longnamesfirst,comma}

% \biboptions{}

\journal{Speech Communication}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
 \title{Active Learning based Constrained Clustering For Speaker Diarization\tnoteref{t1}}
\tnotetext[t1]{This material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. D10PC20024. }
\author{Chengzhu Yu}
\author{John H.L. Hansen\corref{cor1}}
\ead{john.hansen@utdallas.edu}
 \ead[url]{http://crss.utdallas.edu}
 %\fntext[]{}
 \cortext[cor1]{Corresponding author}
\address{Center for Robust Speech Systems (CRSS)\\ Department of Electrical Engineering, The University of Texas at Dallas\\ Richardson, TX 75080-3021, U.S.A. }
%% \fntext[label3]{}
\hyphenation{identification}

\begin{abstract}
Most speaker diarization researches focus on unsupervised scenarios, where no human supervision is available,. However, in many real-world applications, certain amount of human input could always be engaged, especially when minimal human supervision could brings significant performance improvement. Motivated by this, we propose an active learning based speaker clustering algorithm, to effectively improve speaker diarization performance with limited human input. Specifically, proposed algorithm has two different active learning components: \textit{explore} and \textit{consolidate}, acting in initial stages of bottom-up speaker clustering. While the concept of \textit{explore} and \textit{consolidate} are borrowed from the area of data mining, a substantial changes are made here for the application of speaker diarization. The main purpose of \textit{explore} and \textit{consolidate} stages in proposed algorithm, is to boost the speaker clustering process with reliably estimated initial speaker clusters. To achieve this, we uses farthest first query search (FFQS) with active learning to quickly discover at least one sample for each speaker during \textit{explore} phase, and employ nearest neighbor query search (NNQS) during \textit{consolidate} phase to ensure reliable instances for each discovered speaker cluster. After \textit{explore} and \textit{consolidate} phases, the standard bottom-up clustering is performed with a constraint that the clusters discovered during \textit{explore} phases are not merged with each other. Finally, we also propose an active learning based cluster reassignment approach, where clustered segments that accounted for largest expected speaker error are chosen for human evaluation. We evaluate the proposed algorithm on a subset of Apollo multi-track corpus and ICSI meeting corpus. The results indicate significant improvement of speaker diarization performance with relatively small amount of human supervision.     

\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
Speaker diarization \sep active learning  \sep  constrained clustering \sep bottom-up clustering

\end{keyword}

\end{frontmatter}

 %\linenumbers

%% main text
\section{Introduction}
Speaker diarization is the process of automatically detecting \textit{who spoke when }in an audio sequence. With increasing amount of audio resources, speaker diarization becomes an important technology in many applications such as information retrieval, metadata extractions, meeting annotations, and conversation analysis. Recent developments for speaker diarization algorithms have been largely driven by Rich Transcription (RT), where speaker diarization plays a role to provide speaker index and other information to achieve improved speech-to-text transcriptions.

As a sequential process, speaker diarization normally involves several components such as voice activity detection, speaker change detection (segmentation), clustering, and re-segmentation. Among theses components, the core of speaker diarization is clustering, where it separate segments from different audio sources such as speaker, music, and noise, and group them together. Due to its significance in speaker diarization, various solutions have been proposed for speaker clustering. Theses include but not limited to bottom-up approach, also known as agglomerative hierarchical clustering (AHC), top-down approach, and recently proposed global optimization approaches. 

Bottom-up clustering is in general the most popular strategy among various clustering solutions. It starts by treating each individual segments, obtained in the segmentation stage, as separate clusters, and iteratively merging the closest two clusters until a specified stopping criteria is satisfied. While not as popular as its counterpart, top-down approach has also been applied widely and some studies have reported that it could achieve comparable result as bottom-up clustering [Evans, 2012]. Different from bottom-up based approach, top-down clustering starts from modeling the entire audio as single model and iteratively splitting the model into sub clusters until a stopping criteria is met. Despite their differences, both bottom-up and top-down based approach are iterative process and has the drawback of error propagation. A recently proposed clustering algorithm, Interger Linear Programming (ILP), attempts to overcome this drawback by finding the cluster assignments that jointly minimizing the number of assigned clusters as well as within-cluster dispersion. While the ILP could avoid the drawbacks of error propagation, it has to start with initial clusters that contains sufficient samples for estimating its characteristic (e.g., i-vector). Therefore, ILP is mostly performed on top of a bottom-up clustering.

Along with the development in speaker clustering, the distances metric for performing speaker clustering has also made significant improvement from original Bayesian informative criteria (BIC), generalized log-likelihood ration (GLR), Kullback-Leibler (KL) divergence, to recent i-vector based distances such as cosine distance score (CDS) as well as probablistic linear discriminant analysis (PLDA) based distance. Other alternative distance metrics, such as these based on information theoretic framework are also proposed and showed competitive results. 

Despite the success of recent improvement on speaker clustering algorithms, distance computations, as well as other non-trivial components, speaker diarization still remained as challenging tasks in many real-word applications, especially where the audio quality is suboptimal or the speech communications comprises large proportions of fast speaker turns. For example, diarization of telephone conversations are notably more challenging compared with broadcast news diarization, as well as meeting room diarization. 

Due to the limitation within current speaker diarization systems using exclusively audio/speech information, a number of recent studies have proposed to exploit auxilary informations for improved speaker diarization performance. For example, the linguist information such as speaker name occurring patterns are extracted from the speech transcripts to provide additional information during speaker clustering. The speech transcript could be obtained from manual transcript as well as automatic speech recognition system. Another important supplementary information that presents in many speaker diarization applications is the visual information. The audio-visual speaker diarization has also been studied.

In this study, we propose an active learning based, bottom-up speaker clustering algorithm that effectively utilize human input to improve speaker diarization performance. Our proposed algorithm is based on an assumption that human input could be engaged during speaker diarization process in certain applications. This scenario is especially plausible if small amount of human engagement could brings significant performance improvements. Another assumption we made in this paper is that, human is better than machine for recognizing whether a given segment pair is from the same speaker or not. This assumption is based on the results from previous studies that while current speaker recognition systems shows competitive performance as human in clean speech condition, in adverse conditions human significantly outperform machine. Besides, many audio streams for speaker diarization applications contain higher level information such as video, spoken names, and contextual informations that could effectively employed by human for determining whether the two segments are from the same speaker or not.

The format of query for human to provide ground truth in speaker diarization, is \textit{yes or no} type of question asking whether a given segments pair belongs to the same speaker or not. The total number of queries for obtaining perfect clustering results requires $O(N^{2})$, where N indicates the number of segments. Therefore, an effective active query selection strategy is necessary in order to effectively employ human input to boosting the speaker diarization performance. 

To effectively employ human input, we first need to identify which part of speaker clustering components has most effect on final speaker diarization performance. In other words, involving human input in which part of current speaker clustering algorithm, could most effectively improve the overall speaker diarization performance. A recent study by [Simon King] has evaluated several key components of speaker diarization and concluded that initialize speaker models with pure and reliably labeled data could lead to significant improvement to the overall speaker diarization performance.   Motivated by the above findings, we designed our active learning algorithm to quickly discover all or most of the speakers in audio stream in the \textit{explore} phase, and ensure sufficient and reliable samples for each cluster to during \textit{consolidate} phase. The initial speaker models for trained after explore and consolidate phases are much reliable than those obtained in fully unsupervised manner, and are expected to lift the speaker clustering process.

In addition to active learning for improved speaker clustering process, we also investigate the use of active learning for cluster reassignment approach. The objective of proposed cluster reassignment approach, is to actively select certain speech segments from clustered result for human evaluation/correction. The key of active learning based cluster reassignment, is to effectively find most informative segments, from current clustering results. In this study, we find speech segments with largest expected speaker error difference as a candidates for human evaluation.

To summary, in this study, we investigate the use of active learning for bottom-up clustering stage of speaker diarization. We proposed two different strategies where active learning are employed for robust initial cluster estimation and post clustering reassignment, respectively. The remainder of paper is organized as follows.   

\begin{figure*}
	\includegraphics[width=\linewidth]{figs/flow3}
	\caption{A boat.}
	\label{fig:boat1}
\end{figure*}

\section{Related Work}
Active learning based constrained clustering has been extensively studied for image clustering tasks and in broader area of data mining. A number of active learning algorithms have been proposed to use active learning for clustering unlabeled data with human in the loop. The key idea behind these algorithms is to actively select pairs of data for human to provide answer in a form of yes or no. 
Most of the algorithms proposed in these studies, are targeting k-means clustering, and normally composed of two stage: \textit{explore} and \textit{consolidate}. The purpose \textit{explore} phase is to find the centroids of different clusters, and the aim of consolidate stage is to locate most informative data pair for human labeling. While the fundamental problem of these studies is much similar to the problem we have in speaker diarization, active learning for speaker diarization is more challenging task in general due to the two reasons. Firstly, the bottom-up clustering in speaker diarization requires to iteratively clustering two closest segments and update the cluster statistics after each iteration. Therefore, the decisions in current iteration has reliance on previous iteration, and therefor it is difficult to quantify the importance of each segment pair. Another important difference is that, the total number of cluster numbers in speaker diarization are unknown most of time. Due to these differences, the direct replication of active learning algorithms developed in these studies, is not viable.

On the other hand, only a limited number of studies in the area of speaker recognition and diarization, has investigates the use of active learning for speaker diarization tasks. For example, the study by [Shum] has investigate to use active learning to obtain background speaker labels from unlabeled data for training PLDA system. While the study by [Shum] bears similarity with this paper, but the ultimate goal of [Shum] is to locate reliable samples sufficient enough to train PLDA system, rather than clustering entire data as in speaker diairization task. Another study, that employ active learning for speaker diarization is by [Mateusz], where the criteria of active selection for human labeling is simply based on the length of speech, which will not work in many speaker diarization task where the variance of segment length is small.

\section{Baseline System}
The baseline speaker diarization used in this study, is a bottom-up speech clustering algorithm with i-vector cosine distances score as distance metric between two speech segments. In this section, we will give a brief discussion about i-vector extraction, cosine distance score (CDS) and bottom-up clustering approach. 

\subsection{i-vector extraction}
In i-vector extraction framework, speaker and channel dependent GMM supervector is modeled as follows:
\begin{equation}
\begin{aligned}
& M=m+Tw,
\label{mmtw}
\end{aligned}
\end{equation}
where $m$ is the supervector obtained from the universal background model (UBM), 
$T$ is the low rank total variability matrix representing the basis of reduced total variability space, 
and $w$ is the low rank factor loadings referred to as i-Vectors. 

The estimation of the total variability matrix $T$ employs expectation maximization (EM) method as described in \cite{kenny2005eigenvoice}. 
After training the total variability matrix, 
the i-Vector of given speech utterance is extracted as the conditional expectation of i-Vector distribution given observation features.
\begin{equation}
\begin{aligned}
&w_{s}^{\ast}=E[P(w_{s}|X_{s})],
\label{eq:ws}
\end{aligned}
\end{equation}
where $w_{s}^{\ast}$ is the i-Vector of the given speech utterance $s$, 
$X_{s}$ is the clean observation features, 
$P(w_{s}|X_{s})$ is the conditional distribution of the i-Vector given observation features, 
and $E[\cdot]$ indicates the expectation. 
Finally, the i-Vector of the given speech utterance can be represented using the Baum-Welch zeroth ($N_s$) and centralized first ($F_s$) order statistics,
\begin{equation}
\begin{aligned}
&w_{s}^{\ast}=(T'N_{s}\Sigma^{-1}T+I)^{-1}T\Sigma^{-1}F_{s},
\label{ws2}
\end{aligned}
\end{equation}
where $\Sigma$ is the covariance matrix obtained from UBM model and $I$ is the identity matrix.

\subsection{Cosine Distance Score}
Comparison  i-vectors from two different segments, could be successfully achieved with simple cosine similarity. The cosine distance score between two vectors could be expressed as follow:
\begin{equation}
\begin{aligned}
& score (w_i, w_j) = \dfrac{w^T_i \cdot w_j}{\rvert\rvert{w_i}\rvert\rvert \cdot \rvert\rvert{w_j}\rvert\rvert}.
\label{ws2}
\end{aligned}
\end{equation}
Note that, the score of cosine distance ranges between [-1, 1]. The higher the number towards 1, the more similarity exist between two vectors. The cosine distance score has been a popular metric for speaker recognition and verification in i-vector space.
 
\subsection{Bottom-up Speaker Clustering}
Bottom-up clustering, also known as hierarchical agglomerative clustering, has been the most popular speaker clustering algorithm used for speaker diarization. It typically start by treating all homogeneous segments as a separate cluster, and iteratively merge two closest clusters. In our study, for each iteration, we find two segments that has the highest cosine similarity, and merge them into a single cluster. After each iteration, i-vectors of all segments are normalized to have zero mean. We continue the iteration until CDS of two closest segment reached stopping criteria. 

\section{Active Learning for Cluster Initialization}
Due to the error propagation characteristics of bottom-up speaker clustering, having good initial cluster model, with pure speaker segments in each clustering, has significant impact on the final speaker diarization performance. In this section, we will give a detailed description of our proposed active learning strategy for cluster initialization. As mentioned in Introduction, the proposed algorithm has two active learning components: \textit{explore} and \textit{consolidate}.

\subsection{Explore}
The purpose of explore phase, is to quickly discover all the speaker clusters within the audio streams, finding at least one speech segment for each speaker cluster. To achieve this, we use the farthest first query search (FFQS) proposed by [Basu]. Specifically, a speech segment is randomly selected from all speech segments to be used as seed segment. The selected speech segment is then used to initialize the first cluster. After creating the first speaker cluster, the next segment is selected which is farthest from existing clusters. The chosen segment is then provided for human to provide expert opinion. If the chosen segment belongs to existing clusters after pairwise comparison, the new segment is merged to the corresponding cluster. Otherwise, a new cluster is created from the selected segment. Note that, in order to decided if a given speech segment belongs to a target cluster or not, we compose a query pair using the segment in question and the longest segment within the target cluster. If the answer returns by human expert is true, then the segment belongs to the target cluster, and vice versa. The FFQS process continues until the pairwise comparison operations reached specified maximum number specified by user.   
 
\begin{algorithm}
	\KwData{Set of speech segments $X = {\{x_i\}}_{i=1}^n$, acess to the answers of pairwise queries, maximum number queries $Q$ user specified. }
	\KwResult{$C_{k=1}^{k}$ initialized clusters}
	Start from null cluster $C= \{\}$\;
	Select a segment $x$ at random, and create the first cluster as $C_1 = \{x_i\}$, $\lambda \leftarrow 1$\; 
	\While{maximum queries not reached, $\lambda < Q$}{
		Find speech segment $x_{\lambda}$ farthest from existing clusters in $C$, based on i-vector cosine similarity scores.\;
		\eIf{$x_{\lambda}$ belongs to any clusters in $C$}{
			Add speech segment $x_{\lambda}$ to cluster matching cluster
		}{
			Create new cluster $C_k$ with speech segment $x_{\lambda}$\;
		}
		Increase $\lambda$, for each query access. 	
	}
	\caption{FFQS with random seed during \textit{explore} phase.}
\end{algorithm}
 
While the above algorithms could effectively explore the speaker clusters in the audio streams, its performance varies a lot depending on which seed segment is selected. This problem is due to the randomness during seed selection. The previous studies in k-means clustering has revealed that, the initial seed data should be closer to actual centroids in order to achieve favorable clustering results. Motivated by this, we first performed a fully unsupervised bottom-up clustering on the entire speech segments, and taking the centroid segment of each cluster as initial active selection candidates to start FFQS algorithm during \textit{explore} phase.

\subsection{Consolidate}
The FFQS during \textit{explore} stage is very effective in discovering all involved speaker with given audio stream. However, it is not effective, when we need to quickly gather enough instances for each cluster to obtain reliable speaker models. For this purpose, for each discovered clusters obtained from \textit{explore} stage, we uses a nearest neighbor query search (NNQS) approach, to select speech segments closest to it for human evaluation.  

\begin{algorithm}
	\KwData{$C_{k=1}^{k}$ initialized clusters, set of speech segments $X = {\{x_i\}}_{i=1}^n$, acess to the answers of pairwise queries, maximum number queries per cluster $Q_k$ user specified. }
	\KwResult{$C_{k=1}^{k}$ with more instances.}
	\For{each cluster $C_k$ in $C_{k=1}^{k}$}{
		\While{maximum queries per cluster $Q_k$ not reached}{
			Find speech segment $\textbf{x}$ closest from $C_k$, based on i-vector cosine similarity scores.\;
			\eIf{$\textbf{x}$ belongs to any clusters in $C_k$}{
				Add speech segment $\textbf{x}$ to cluster $C_k$
			}{
				Exlude this segment $\textbf{x}$ from search in next iteration.
			}
		}
	}
	\caption{NNQS algorithm during \textit{consolidate} phase.}
\end{algorithm}

\subsection{Constrained Merging}
After \textit{explore} and \textit{consolidate} phases, the standard bottom-up clustering is performed with two important exceptions. First, the clusters $C_{k=1}^{k}$ created during \textit{explore} and \textit{consolidate} stages, are restricted to not merge with each other. Second, the stopping distance threshold during bottom-up clustering is no longer necessary, as we have assumed all involved speakers are discovered during \textit{explore} phase. The bottom-up clustering will continues until only $C_{k=1}^{k}$ clusters remain.

\begin{figure*}
	\includegraphics[width=\linewidth]{figs/flow4}
	\caption{A boat.}
	\label{fig:boat1}
\end{figure*}

\section{Active Learning for Cluster Reassignment}
In previous section, we propose an active learning based algorithm to obtain better initial speaker models. Alternatively, human input could be involved after clustering finishes, to evaluate and fix incorrectly assigned clusters. This is quite similar to the use of active learning for automatic speech recognition (ASR), where the sentence with less confidence is passed to human for correction. However, the same algorithm used for ASR could not be directly applied in speaker diarization for several reasons. First, the confidence measure used for ASR is not appropriate for speaker diarization. Second, assuming we have identified potential erroneous segments, the human evaluation for evaluate and reassign these segments are much more complex than human transcription in ASR.

The proposed active learning based cluster reassignment has three major components that we will explain in this section. 

\subsection{Candidates selection}
The cluster reassignment algorithm starts with selecting the candidate speech segments and its assigned cluster for human expert to review later. In order to effectively select most informative segments, we ranks all speech segments in terms of expected speaker error reduction. In other words, we will select the speech segments that will produce largest expected speaker error reduction. 
The expected speaker error for each speech segment could be computed as below. 
\begin{equation}
\begin{aligned}
E(x_j) =& P(x_j | C_j) \cdot J_{x_j \in C_j} \\
&+ (1 - P(x_j | C_j)) \cdot J_{x_j \notin C_j}
\label{expect}
\end{aligned}
\end{equation}

where $x_j$ indicates $j$th speech segment, $C_j$ is the cluster assigned to speech segment $x_j$, $P(x_j|C_j)$ is the probability of segment $x_j$ belongs to cluster $c_j$, $J_{x_j \in C_j} $ is the speaker error if $x_j$ belongs to cluster $C_j$, and $J_{x_j \notin C_j}$ is the speaker error if $x_j$ not belongs to cluster $C_j$. We could also write that
\begin{equation}
\begin{aligned}
&J_{x_j \in C_j} = 0 \\
&J_{x_j \notin C_j} = \dfrac{d_j}{\sum_{i=1}^{n} d_i}
\label{jj}
\end{aligned}
\end{equation} 
where $d_j$ is the length of speech segment $x_j$, and $\sum_{i=1}^{n} d_i$ is total length sum of all speech segments of the testing audio stream.

We compute $P(x_j|C_j)$ by modeling a multivariate Gaussian distribution for each $C_{k=1}^{k}$ using i-vectors of all the segments of given cluster. We normalize the probability to sum to one.
\begin{equation}
\begin{aligned}
&  \sum_{i=1}^{k} P(x_j|C_i) = 1;
\label{jj}
\end{aligned}
\end{equation}
 

\subsection{Evaluation}
After selecting the candidate segments, human expert will determine whether the given segment belongs to its assigned cluster. The process of employing human input to decide whether a segment belongs to a cluster, is more difficult than the strategy we used in \textit{explore} and \textit{consolidate} phases of previous section. We could not compose query pair with the segment in question and a single longest segment in the cluster. This is due to the fact that the chosen longest segment of the cluster, could also be an incorrect assignment. Therefore, we employ a majority voting based segment cluster evaluation strategy. Under this strategy, the segment in question is paired with each segments within assigned cluster, resulting in multiple query pairs. If majority answers of these pairs are true (two segments belong to the same cluster), we will make a decision that the given speech segment has correct cluster assignment, and vice versa. 

While the above strategy is robust for evaluating whether a given segment belongs to target cluster, it involves significant number of query pair for human evaluation. One heuristic that we used in our study to effectively reduce the query pair number is to set a maximum query number limit $V$ for each segment evaluation. We rank each segments with the target cluster by its confidence $P(x|c)$, and select top $V$ confident segments for composing pairs with test segment. 

\subsection{Correction}
After detecting segments with incorrect cluster assignment in \textit{evaluation} stage, we need find the correct cluster designation for these segments. To achieve, we employ N-best cluster evaluation.
We find the $N$ most possible cluster candidates for given segment by ranking the i-vector Gaussian posterior probabilities $P(x|C)$. The human expert will evaluate in order whether the given segment belongs to any of these N clusters, using the majority voting scheme as in the \textit{evaluation} stage. 

\section{Evaluation Data}
We performs experiments on two different speech database: Apollo-11 Mission Control Center (MCC) audio  corpus and ICSI meeting corpus. 

\subsection{Apollo-11 MCC Audio Corpus}
During the NASA Apollo mission, all communications between astronauts, flight controllers inside mission control center (MCC), and their backroom support teams are continuously recorded using a 30-track analog reel-to-reel recording machine. During each mission, a total of 60 audio channels are simultaneously recorded including the voices from more than hundred of different participants of the mission. The University of Texas at Dallas (UTD), University of Maryland College Park (UMD), and Johnson Space Center(JSC) has combined the effort to digitize this data resource and have generated up to 19,000 hours of audio data from various missions of both Apollo and Gemini missions. Those mission audio records the full detail of the Apollo communication, therefore extremely attractive for learning human-to-human communications, group interaction, as well as developing robust speech system. 
\begin{figure}[t]
	\includegraphics[width=\linewidth]{figs/comm5}
	\caption{A boat.}
	\label{fig:boat1}
\end{figure}
Moreover, as the speech community (including us) rely on labeled audio data to perform scientific research as well as algorithmic development, we have prepared a 'Task Specific' corpus based on a subset of Apollo-11 audio recordings. We performed our experiment on this subset of Apollo-11 audio recordings includes 3 synchronized channels(Flight director loop, EECOM loop, GNC) spanning 10-hours before, and after lunar landing. This initial 30 hour task corpus has been transcribed to have speaker labels by well-trained speech science students from UTDallas \footnote{The task corpus will be released to the speech community for research and algorithmic development.}.

The audios in Apollo-11 MCC audio corpus includes two types of communication: space-to-ground communications between astronauts and CAPCOM, and ground communication between flight directors and "backroom" support staff. Most of the audios are recorded with close-talking microphones and the audios are in general good quality. The audios from astronauts transmitted through Earth's dedicated telephone channels to Houston from ground stations where the signal was received. The flight directors as well as their "backroom" supports' voice are recorded through intercom circuit called "loops". Each flight director has their own loops, which records the entire communication within that channel.

The voice communication style withing Apollo mission control center is much different from both traditional meeting corpus, including many short speech segments which was intended to improve communication efficiency. 



\subsection{ICSI Meeting Corpus}

\section{Experiments and Results}
In this section, we will perform experiments to evaluate proposed active learning based algorithms for speaker diarization. 
\subsection{System Setup}
\subsection{Active Learning Based Cluster Initialization}
\subsection{Active Learning Based Cluster Reassignment}
%% References with bibTeX database:

\bibliographystyle{elsarticle-harv}
\bibliography{IEEEabrv,MOREabrv,spcomrefs}

%% Authors are advised to submit their bibtex database files. They are
%% requested to list a bibtex style file in the manuscript if they do
%% not want to use elsarticle-harv.bst.

%% References without bibTeX database:

% \begin{thebibliography}{00}

%% \bibitem must have one of the following forms:
%%   \bibitem[Jones et al.(1990)]{key}...
%%   \bibitem[Jones et al.(1990)Jones, Baker, and Williams]{key}...
%%   \bibitem[Jones et al., 1990]{key}...
%%   \bibitem[\protect\citeauthoryear{Jones, Baker, and Williams}{Jones
%%       et al.}{1990}]{key}...
%%   \bibitem[\protect\citeauthoryear{Jones et al.}{1990}]{key}...
%%   \bibitem[\protect\astroncite{Jones et al.}{1990}]{key}...
%%   \bibitem[\protect\citename{Jones et al., }1990]{key}...
%%   \harvarditem[Jones et al.]{Jones, Baker, and Williams}{1990}{key}...
%%

% \bibitem[ ()]{}

% \end{thebibliography}

\end{document}

%%
%% End of file `elsarticle-template-harv.tex'.
